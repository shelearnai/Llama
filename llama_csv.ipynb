{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 09:33:15.340748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 09:33:16.251322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b67b20981a42849b76abb9ca9d2e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between Age and Fare is 0.11\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a tool for calculating correlation\n",
    "def calculate_correlation(df, col1, col2):\n",
    "    correlation = df[col1].corr(df[col2])\n",
    "    return f\"The correlation between {col1} and {col2} is {correlation:.2f}\"\n",
    "\n",
    "class LLMWithTools:\n",
    "    def __init__(self, llm_pipeline):\n",
    "        self.llm_pipeline = llm_pipeline\n",
    "        self.tools = {}\n",
    "\n",
    "    def bind_tools(self, tools):\n",
    "        for tool in tools:\n",
    "            self.tools[tool.__name__] = tool\n",
    "\n",
    "    def invoke(self, prompt, **kwargs):\n",
    "        # Check if the prompt matches any tool's function\n",
    "        for tool_name, tool_func in self.tools.items():\n",
    "            if tool_name in prompt:\n",
    "                # Extract the arguments for the tool from kwargs\n",
    "                return tool_func(**kwargs)\n",
    "        \n",
    "        # If no tool matches, use the LLM pipeline\n",
    "        response = self.llm_pipeline(prompt, max_length=100)\n",
    "        return response[0]['generated_text']\n",
    "\n",
    "# Create the LLMWithTools instance and bind the correlation tool\n",
    "llm_with_tools = LLMWithTools(llm_pipeline)\n",
    "llm_with_tools.bind_tools([calculate_correlation])\n",
    "\n",
    "# Load your CSV data\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Invoke the tool through the LLMWithTools instance\n",
    "response = llm_with_tools.invoke(\n",
    "    \"calculate_correlation\",\n",
    "    df=df,\n",
    "    col1='Age',\n",
    "    col2='Fare'\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc331dc1a4b1497a8d32d2e55ad06d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a planner who will plan the solution to the question --> What is the correlation between the 'Age' and 'Fare' columns?\n",
      "\n",
      "You are working on this data:\n",
      " Age  Fare  Survived\n",
      "  22  7.25         1\n",
      "  30 71.83         0\n",
      "  24  8.05         1\n",
      "  45  8.05         1\n",
      "  36  8.05         0\n",
      "  50 51.86         1\n",
      "\n",
      "Based on the user's question, come up with a detailed plan in plain English which is an exhaustive step-by-step plan to solve the problem.\n",
      "Try to put maximum steps from your side to help the user.\n",
      "This plan should involve individual tasks that, if executed correctly, will yield the correct answer.\n",
      "Do not add any superfluous steps.\n",
      "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
      "\n",
      "Here are all the columns of the Data file. It will be helpful for you to understand the column details:\n",
      "Age, Fare, Survived\n",
      "Here is a sample of the data:\n",
      "Age  Fare  Survived\n",
      "  22  7.25         1\n",
      "  30 71.83         0\n",
      "  24  8.05         1\n",
      "  45  8.05         1\n",
      "  36  8.05         0\n",
      "  50 51.86         1\n",
      "\n",
      "Here is the detailed plan:\n",
      "\n",
      "Step 1: Read the Data\n",
      "Read the data provided in the problem statement. You can do this by creating a pandas DataFrame object in Python. The data is already provided in the problem statement.\n",
      "\n",
      "Step 2: Understand the Data\n",
      "Understand the data provided in the problem statement. In this case, we have three columns: 'Age', 'Fare', and 'Survived'. The 'Age' column represents the age of the passengers, the 'Fare' column represents the fare paid by the passengers, and the 'Survived' column represents whether the passengers survived or not.\n",
      "\n",
      "Step 3: Check for Missing Values\n",
      "Check for missing values in the data. In this case, there are no missing values.\n",
      "\n",
      "Step 4: Calculate the Correlation\n",
      "Calculate the correlation between the 'Age' and 'Fare'\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame for demonstration\n",
    "data = {\n",
    "    'Age': [22, 30, 24, 45, 36, 50],\n",
    "    'Fare': [7.25, 71.83, 8.05, 8.05, 8.05, 51.86],\n",
    "    'Survived': [1, 0, 1, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract column names and format data\n",
    "columns = \", \".join(df.columns.tolist())\n",
    "data_string = df.to_string(index=False)\n",
    "\n",
    "# Define the question\n",
    "question = \"What is the correlation between the 'Age' and 'Fare' columns?\"\n",
    "\n",
    "# Define the PromptTemplate\n",
    "class PromptTemplate:\n",
    "    def __init__(self, input_variables, metadata, template):\n",
    "        self.input_variables = input_variables\n",
    "        self.metadata = metadata\n",
    "        self.template = template\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "# Create the prompt template\n",
    "template_string = '''\n",
    "You are a planner who will plan the solution to the question --> {question}\n",
    "\n",
    "You are working on this data:\n",
    "{data}\n",
    "\n",
    "Based on the user's question, come up with a detailed plan in plain English which is an exhaustive step-by-step plan to solve the problem.\n",
    "Try to put maximum steps from your side to help the user.\n",
    "This plan should involve individual tasks that, if executed correctly, will yield the correct answer.\n",
    "Do not add any superfluous steps.\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Here are all the columns of the Data file. It will be helpful for you to understand the column details:\n",
    "{columns}\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['columns', 'data', 'question'],\n",
    "    metadata={'lc_hub_owner': 'lk-ml', 'lc_hub_repo': 'csv_file_reader', 'lc_hub_commit_hash': 'b98687798ee3bf8e3896b4affde9208da4237007512bb3efdbff55222849adee'},\n",
    "    template=template_string\n",
    ")\n",
    "\n",
    "# Format the prompt with the context\n",
    "formatted_prompt = prompt_template.format(\n",
    "    columns=columns,\n",
    "    data=data_string,\n",
    "    question=question\n",
    ")\n",
    "\n",
    "# Generate a response using the LLM pipeline\n",
    "response = llm_pipeline(formatted_prompt, max_length=500)\n",
    "\n",
    "# Print the response\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
