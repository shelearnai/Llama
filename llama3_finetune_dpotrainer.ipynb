{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10328d92-70ca-4e1a-b54f-ad2d92778163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7822a3e449444f0eb972134be3db0972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d178cfd7-1236-4c31-9de0-90b667f3684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 09:33:36.703468: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 09:33:38.455112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    PeftModel, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import DPOTrainer, setup_chat_format\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc57af-8831-4cf3-a174-28a974820aaf",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n",
    "\n",
    "Note that we also load in a reference model. This is for completeness. If we did not provide one, the DPOTrainer will automatically create one for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4c7700-425b-4b78-9766-bd74cce5983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"DPOLlama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de73f0-ce69-4621-bb05-8778fa89a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bits and bytes config we use for quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efabe9fd-c0d7-410a-a46a-1c53febebfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b51d253abe4fbc9edb336aaf1e4723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5d0b447a484c7c80744f9b5f36f1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133104d5a00043efa7ad0a103dcbbcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051ae369d1cf43bc84f8c81c251a8f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d819a818a644c6978da6db4a7e1f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a631fdb5ec3427abdfae564b4b8bbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e4c4f1ffd44bbabafa80e66ab50f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88840d87213f41968a1f0171991fc94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bf7708f3854967a7a6d3e30869e5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22080622b9f44ecb4db1b0deef2e00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1315a057eec14c0caaa95a307208650b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f26429229e947eeb823cc5b936700e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ba503f48f54f85b53c34480867cf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# Reference model\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030e7082-2d8d-4e65-87c9-e35bd7485e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b2d9a-c63d-43c5-8c1a-31df908c77e8",
   "metadata": {},
   "source": [
    "## Load and format dataset\n",
    "\n",
    "Preference datasets are a little unique as I explained above. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaecd90-37d8-4446-aff7-7a86cd2fc9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af3a0abd2914937980933478c86a5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/196 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17a155b25ec4b0aa22c1e3b910fd876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/36.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b2a4e9ccf747f29685d2143ac50ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Intel/orca_dpo_pairs\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69345d25-aad7-4c56-8a83-487a4cdaf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we use a subset of 150 samples. In order to do a full tune, you can use around 1000 or the entire dataset\n",
    "# The seed allows for reproducability. If you run this notebook exactly, you will use the same 150 samples as me\n",
    "dataset = dataset.shuffle(seed=42).select(range(150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02dcad7-2bb4-448d-bc41-af42bea4f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'question', 'chosen', 'rejected'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415d8e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'You are an AI assistant that follows instruction extremely well. Help as much as you can.',\n",
       " 'question': 'What key details about al-walid i  can be extracted from the following bio?  Bio: al-walid ibn abd al-malik -lrb- -rrb- or al-walid i -lrb- 668 -- 23 february 715 -rrb- was an umayyad caliph who ruled from 705 until his death in 715 . his reign saw the greatest expansion of the caliphate , as successful campaigns were undertaken in transoxiana , sind , hispania and against the byzantines .\\n',\n",
       " 'chosen': '1. Name: Al-Walid ibn Abd al-Malik or Al-Walid I\\n2. Birth year: 668\\n3. Death date: 23 February 715\\n4. Title: Umayyad Caliph\\n5. Reign: Ruled from 705 to 715\\n6. Key accomplishment: His reign saw the greatest expansion of the Caliphate\\n7. Successful campaigns: Undertaken in Transoxiana, Sind, Hispania, and against the Byzantines',\n",
       " 'rejected': \" Sure, I can help you extract some key details about al-Walid ibn Abd al-Malik from the given bio. Here are some of the information that can be gathered:\\n\\n1. Name: al-Walid ibn Abd al-Malik\\n2. Title: Umayyad Caliph\\n3. Reign: 705-715 CE\\n4. Date of Birth: 668 CE (approximate)\\n5. Date of Death: February 23, 715 CE\\n6. Place of Death: (not specified in the bio)\\n7. Achievements:\\n\\t* Greatest expansion of the caliphate\\n\\t* Successful campaigns in:\\n\\t\\t+ Transoxiana\\n\\t\\t+ Sind\\n\\t\\t+ Hispania\\n\\t\\t+ Against the Byzantines\\n\\nNote that the bio does not provide information on al-Walid's father's name, mother's name, or any other family members. Additionally, there is no information on his early life, education, or any significant events that occurred during his reign.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5feec5d9-64d4-4f8f-bb2e-7a8f840718a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WWE star invites Conor McGregor to join WWE\\nConor McGregor turned the UFC world upside down in less than 140 characters Tuesday, cryptically tweeting that he has made the decision to \"retire young.\"\\n\\nI have decided to retire young.Thanks for the cheese. Catch ya\\'s later.\\n\\nNo one really knows what McGregor means. He may have just announced his retirement at the age of 27, or perhaps McGregor is simply sharing that he doesn\\'t plan to fight well into his 30s. As one of the biggest stars in sports, and at the height of his earning potential, it would seem crazy for McGregor to actually retire - but he would certainly have lucrative options outside of the octagon.\\n\\nMcGregor\\'s Irish compatriot Becky Lynch suggested on Twitter that the UFC superstar should make the jump to professional wrestling and join WWE.\\n\\nMcGregor wouldn\\'t be the first MMA star to transition to WWE. \"The World\\'s Most Dangerous Man\" Ken Shamrock is the best example of a success story. Shamrock came to WWE in 1997 as a former UFC Superfight Champion and became a star over a short two-year span, winning the Intercontinental title and the 1998 King of the Ring.\\n\\nMMA legend Dan Severn held the UFC Superfight title and the NWA heavyweight title simultaneously in 1995, and had a short run in WWE in the late 90s. Alberto Del Rio and Shinsuke Nakamura each had short MMA careers before stepping into a WWE ring, while Brock Lesnar became the UFC heavyweight champion after his first stint as a pro wrestler.\\n\\nWould McGregor work for WWE? He\\'d certainly be a massive draw, but would also undoubtedly clash with everyone else on the roster. McGregor has trained as a boxer and a fighter, but not as a pro wrestler - and it would be interesting to see if WWE would have McGregor remain a fighter (think Floyd Mayweather vs. The Big Show at WrestleMania) or have him actually learn how to wrestle.\\n\\nThere\\'s also the challenge of his character. McGregor has one of the biggest personalities in sports, and it would almost be comical to see him in any storyline that doesn\\'t revolve around the WWE title. McGregor\\'s such a big star that he would have to be treated as such, but that limits who he could believably face in WWE. As a five-matches-per-year, Brock Lesnar-type of attraction, McGregor could make for some must-watch pay-per-views, but it\\'s hard to see him working as a weekly Raw main eventer.\\n\\nAlternatively, WWE could bring back the Lion\\'s Den.\\nWhat was this article about?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[19]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0535908f-62ae-4136-a4c3-2742b8b51da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The article discusses the possibility of Conor McGregor, a UFC star, joining WWE. McGregor has announced his retirement from the UFC, and WWE star Becky Lynch has suggested that he should join WWE. The article explores the potential of McGregor joining WWE, including the possibility of him being a massive draw, but also notes that it would be challenging to integrate him into the WWE roster and storylines. The article also mentions other MMA stars who have transitioned to WWE, such as Ken Shamrock and Brock Lesnar. Additionally, the article suggests that McGregor's character and personality would be a major factor in any WWE storyline.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[19]['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43188647-62b8-4d99-96da-bb689a71cb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article is about the speculation surrounding UFC star Conor McGregor\\'s potential retirement from the sport after his cryptic tweet hinting at \"retiring young.\" The article discusses the suggestion made by McGregor\\'s Irish compatriot Becky Lynch that he should transition from UFC to professional wrestling, specifically WWE. The possibility of McGregor joining WWE is analysed, drawing comparisons with other MMA stars who have made the switch, such as Ken Shamrock, Dan Severn, Alberto Del Rio, Shinsuke Nakamura, and Brock Lesnar.\\n\\nThe article addresses various factors that would need to be considered, such as McGregor\\'s professional wrestling training, potential character challenges, and the scope of his involvement in WWE storylines. It concludes by contemplating the idea of McGregor taking part in occasional high-profile WWE pay-per-view events, similar to Brock Lesnar\\'s approach, rather than becoming a regular main eventer.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[19]['chosen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663a837-21da-4477-aedd-e2d320b6af75",
   "metadata": {},
   "source": [
    "Now we format the dataset in order to follow the Llama3 format using the extremely helpful chat templates created by chujiezheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a71a1a3-9e74-453d-9949-128e0fcbfea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-16 09:42:04--  https://raw.githubusercontent.com/chujiezheng/chat_templates/main/chat_templates/llama-3-instruct.jinja\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 598 [text/plain]\n",
      "Saving to: â€˜llama-3-instruct.jinjaâ€™\n",
      "\n",
      "llama-3-instruct.ji 100%[===================>]     598  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-16 09:42:05 (35.5 MB/s) - â€˜llama-3-instruct.jinjaâ€™ saved [598/598]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -L https://raw.githubusercontent.com/chujiezheng/chat_templates/main/chat_templates/llama-3-instruct.jinja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b3ee67-c884-4e47-a037-862598fd1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = open('llama-3-instruct.jinja').read()\n",
    "chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb384d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e598f2c-215a-4cb3-9211-63f5a8de1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_format(example):\n",
    "    # Format system\n",
    "    if len(example['system']) > 0:\n",
    "        message = {\"role\": \"system\", \"content\": example['system']}\n",
    "        system = tokenizer.apply_chat_template([message], tokenize=False)\n",
    "    else:\n",
    "        system = \"\"\n",
    "    # Format instruction\n",
    "    message = {\"role\": \"user\", \"content\": example['question']}\n",
    "    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "    # Format chosen answer\n",
    "    chosen = example['chosen'] + \"<|eot_id|>\\n\"\n",
    "    # Format rejected answer\n",
    "    rejected = example['rejected'] + \"<|eot_id|>\\n\"\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79b29cc0-0ce0-4dbd-939c-332759ec59ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8094b17e5f494e0e8b6cad1d141625a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_columns = dataset.column_names\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "dataset = dataset.map(\n",
    "    dataset_format,\n",
    "    remove_columns=original_columns,\n",
    "    num_proc= os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78a6478-4983-413e-a3a1-75f4047ca217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'This article is about the speculation surrounding UFC star Conor McGregor\\'s potential retirement from the sport after his cryptic tweet hinting at \"retiring young.\" The article discusses the suggestion made by McGregor\\'s Irish compatriot Becky Lynch that he should transition from UFC to professional wrestling, specifically WWE. The possibility of McGregor joining WWE is analysed, drawing comparisons with other MMA stars who have made the switch, such as Ken Shamrock, Dan Severn, Alberto Del Rio, Shinsuke Nakamura, and Brock Lesnar.\\n\\nThe article addresses various factors that would need to be considered, such as McGregor\\'s professional wrestling training, potential character challenges, and the scope of his involvement in WWE storylines. It concludes by contemplating the idea of McGregor taking part in occasional high-profile WWE pay-per-view events, similar to Brock Lesnar\\'s approach, rather than becoming a regular main eventer.<|eot_id|>\\n',\n",
       " 'rejected': \" The article discusses the possibility of Conor McGregor, a UFC star, joining WWE. McGregor has announced his retirement from the UFC, and WWE star Becky Lynch has suggested that he should join WWE. The article explores the potential of McGregor joining WWE, including the possibility of him being a massive draw, but also notes that it would be challenging to integrate him into the WWE roster and storylines. The article also mentions other MMA stars who have transitioned to WWE, such as Ken Shamrock and Brock Lesnar. Additionally, the article suggests that McGregor's character and personality would be a major factor in any WWE storyline.<|eot_id|>\\n\",\n",
       " 'prompt': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWWE star invites Conor McGregor to join WWE\\nConor McGregor turned the UFC world upside down in less than 140 characters Tuesday, cryptically tweeting that he has made the decision to \"retire young.\"\\n\\nI have decided to retire young.Thanks for the cheese. Catch ya\\'s later.\\n\\nNo one really knows what McGregor means. He may have just announced his retirement at the age of 27, or perhaps McGregor is simply sharing that he doesn\\'t plan to fight well into his 30s. As one of the biggest stars in sports, and at the height of his earning potential, it would seem crazy for McGregor to actually retire - but he would certainly have lucrative options outside of the octagon.\\n\\nMcGregor\\'s Irish compatriot Becky Lynch suggested on Twitter that the UFC superstar should make the jump to professional wrestling and join WWE.\\n\\nMcGregor wouldn\\'t be the first MMA star to transition to WWE. \"The World\\'s Most Dangerous Man\" Ken Shamrock is the best example of a success story. Shamrock came to WWE in 1997 as a former UFC Superfight Champion and became a star over a short two-year span, winning the Intercontinental title and the 1998 King of the Ring.\\n\\nMMA legend Dan Severn held the UFC Superfight title and the NWA heavyweight title simultaneously in 1995, and had a short run in WWE in the late 90s. Alberto Del Rio and Shinsuke Nakamura each had short MMA careers before stepping into a WWE ring, while Brock Lesnar became the UFC heavyweight champion after his first stint as a pro wrestler.\\n\\nWould McGregor work for WWE? He\\'d certainly be a massive draw, but would also undoubtedly clash with everyone else on the roster. McGregor has trained as a boxer and a fighter, but not as a pro wrestler - and it would be interesting to see if WWE would have McGregor remain a fighter (think Floyd Mayweather vs. The Big Show at WrestleMania) or have him actually learn how to wrestle.\\n\\nThere\\'s also the challenge of his character. McGregor has one of the biggest personalities in sports, and it would almost be comical to see him in any storyline that doesn\\'t revolve around the WWE title. McGregor\\'s such a big star that he would have to be treated as such, but that limits who he could believably face in WWE. As a five-matches-per-year, Brock Lesnar-type of attraction, McGregor could make for some must-watch pay-per-views, but it\\'s hard to see him working as a weekly Raw main eventer.\\n\\nAlternatively, WWE could bring back the Lion\\'s Den.\\nWhat was this article about?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice the specific llama3 tags like <|eot_id|> which show that the chat template formatting worked\n",
    "dataset[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9828e-7b0b-434b-8525-0cd504020348",
   "metadata": {},
   "source": [
    "## Create the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e01d00-7d29-4fe7-8e29-a99abc28b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaec96b9-b1aa-4007-8608-9c168d03dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=50, #tweak this to change # of steps in the training run\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    output_dir=new_model,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=10,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "780c8070-b348-4ce3-bacf-6f229c9f8258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29947f683a0b4572a4656a396cf90cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024, \n",
    "    force_use_ref_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "022a71b3-5f02-4bd8-9386-494390f73022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 05:14, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.19132064570963847, metrics={'train_runtime': 321.0201, 'train_samples_per_second': 1.246, 'train_steps_per_second': 0.156, 'total_flos': 0.0, 'train_loss': 0.19132064570963847, 'epoch': 2.6666666666666665})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune model with DPO\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a038e-2a86-4aa0-93b2-2319e9a12a87",
   "metadata": {},
   "source": [
    "## Save and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17124220-fef6-43dc-8ee3-d2453488a4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/gvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer.model.save_pretrained(\"final_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e370e6cb-3bab-4847-b74a-d293bc146c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('final_ckpt/tokenizer_config.json',\n",
       " 'final_ckpt/special_tokens_map.json',\n",
       " 'final_ckpt/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"final_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7174ae27-aef3-4a2d-aeaa-2787ed1e1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "del dpo_trainer, model, ref_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "036fd959-835d-4536-9b5b-ebadaae8aadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e28ef1471c4437e93644b000b5e3fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690457930c2749a78cd9289f91bbeacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392a4a990c2b49ff898875c7c91eef58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d31008b7ae4b9c8a29f7e46efa9c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df5959bcfe34b4c90d1191cd171e484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cfb04205d34e3e9c86cbaef0568a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326614ed4cd14889beb976b73eaf0019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fc6b720e0b42da91ddb8333c02e701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768303031047437eb7aa056b690a1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 (instead of NF4)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8179302-6495-4867-a430-03c4c90559f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90309400c48c4d88b6220aa7e85f93ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa657775d2cf4b4f8f882eebba7c3aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba9b6c6da3d451d92f485e5b22f829e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89b6434f-54ce-4580-bd77-94bc21389d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DPOLlama-3-8B/tokenizer_config.json',\n",
       " 'DPOLlama-3-8B/special_tokens_map.json',\n",
       " 'DPOLlama-3-8B/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge base model with the adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"final_ckpt\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd1a0bf1-bf07-47a5-add1-3c59b6f44ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043841e580fd4d9e8393c2c912c73124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=new_model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fb617ef-a683-4bdb-af45-dc036883c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant chatbot that provides concise answers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are GPUs and why would I use them for machine learning tasks?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "GPUs (Graphics Processing Units) are specialized computer chips designed for high-performance computing, particularly for graphics rendering. They have evolved to also excel in accelerating general-purpose computing, including machine learning tasks.\n",
      "\n",
      "GPUs offer several advantages for machine learning:\n",
      "\n",
      "1. **Parallel processing**: GPUs have thousands of cores, allowing them to perform many calculations simultaneously. This parallel processing capability enables GPUs to handle large datasets and complex computations more efficiently than CPUs (Central Processing Units).\n",
      "2. **Massive memory bandwidth**: GPUs have a high memory bandwidth, which enables fast data transfer between the GPU and system memory. This is crucial for machine learning tasks that require frequent data access.\n",
      "3. **Optimized architecture**: GPUs are designed to handle matrix multiplications, which are a fundamental operation in many machine learning algorithms.\n",
      "\n",
      "Using GPUs\n"
     ]
    }
   ],
   "source": [
    "# Format prompt\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot that provides concise answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are GPUs and why would I use them for machine learning tasks?\"}\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
    "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=200,\n",
    ")\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa36863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
